{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üí° What is Guardrails-AI?\n",
        "\n",
        "**Guardrails-AI** is a powerful framework designed to ensure the safe, ethical, and reliable operation of AI systems‚Äîespecially large language models (LLMs). It works by **validating and filtering** inputs and outputs of AI models to prevent toxic, biased, or unsafe responses.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Benefits of AI Guardrails**\n",
        "\n",
        "### 1. üîê **Safety & Reliability**\n",
        "- **Prevent Harm**: Reduces risk of biased, offensive, or unsafe AI behavior.\n",
        "- **Error Control**: Keeps AI responses within safe and expected boundaries.\n",
        "\n",
        "### 2. ‚öñÔ∏è **Ethical Compliance**\n",
        "- **Bias Detection**: Identifies and mitigates discriminatory or unfair responses.\n",
        "- **Transparency**: Makes AI decisions explainable and auditable.\n",
        "\n",
        "### 3. üìú **Regulatory Adherence**\n",
        "- **Legal Compliance**: Helps meet data privacy laws like GDPR.\n",
        "- **Risk Management**: Reduces chances of legal and reputational issues.\n",
        "\n",
        "### 4. ü§ù **User Trust**\n",
        "- **Predictable Behavior**: Consistent outputs increase user confidence.\n",
        "- **Privacy Protection**: Filters out personally identifiable information (PII) and secret data.\n",
        "\n",
        "### 5. üöÄ **Operational Efficiency**\n",
        "- **Safe Deployment**: Allows confident roll-out of AI applications.\n",
        "- **Scalable Control**: Ensures responsible scaling of AI systems.\n",
        "\n",
        "### 6. üåê **Reputation Management**\n",
        "- **Avoid Failures**: Prevents harmful or viral missteps.\n",
        "- **Build Trust**: Promotes responsible AI adoption and public trust.\n",
        "\n",
        "### 7. üîÅ **Adaptability**\n",
        "- **Dynamic Updates**: Easily adjusts to new policies or ethical standards.\n",
        "- **Context-Aware**: Can tailor responses based on different use cases or environments.\n",
        "\n",
        "### 8. üí° **Supports Innovation**\n",
        "- **Safe Experimentation**: Encourages AI development within ethical and safe frameworks.\n",
        "- **Custom Validators**: Developers can build app-specific checks using the Guardrails Hub.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ°Ô∏è Why Use Guardrails-AI?\n",
        "\n",
        "- ‚úÖ Validates both inputs & outputs of LLMs\n",
        "- üßº Filters toxic content, PII, secrets, and more\n",
        "- üõ†Ô∏è Highly customizable with a rich set of validators\n",
        "- üîí Ensures ethical, secure, and user-friendly AI behavior\n",
        "- üìö Backed by an active Guardrails Hub for community-built components\n",
        "\n"
      ],
      "metadata": {
        "id": "XvkXbeK5nsaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí¨ Chatbot\n",
        "\n",
        "Guardrails can easily be integrated into flows for chatbots to help protect against common unwanted output like profanity and toxic language.\n",
        "\n"
      ],
      "metadata": {
        "id": "eRQv9vvyoTku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After configuring the **Guardrails-AI API key**, you are supposed to install the necessary **validators from the Guardrails Hub**. These validators act as safeguards that can detect and filter out unwanted content such as **profanity**, **toxic language**, and other harmful outputs.\n",
        "\n",
        "We‚Äôll also be using **Gradio** to build a simple interface for interacting with the chatbot."
      ],
      "metadata": {
        "id": "9elLapbKpJL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö†Ô∏è Testing LLM Responses Without Guardrails-AI\n",
        "\n",
        "Below is a chatbot using the OpenAI GPT-4o model **without any safety filters or validation**. We'll test it with some prompts that include **mild profanity**, **sarcasm**, and **toxic language** to see how it responds by default.\n",
        "\n",
        "These are example prompts we'll try:\n",
        "\n",
        "```\n",
        "1. Write a short story where a character uses a mild curse word once.\n",
        "2. Generate a funny insult in Shakespearean style.\n",
        "3. Create a pirate's angry speech after losing his treasure.\n",
        "4. Write a frustrated detective‚Äôs reaction to losing key evidence.\n",
        "5. Make a medieval knight curse after falling off his horse.\n",
        "6. Generate a sarcastic AI assistant‚Äôs response to a rude user.\n",
        "\n",
        "```\n",
        "\n",
        "üëâ Use the chatbot below to input each of these prompts and observe the **raw output from the LLM**. This will help you compare with Guardrails-enabled results later.\n"
      ],
      "metadata": {
        "id": "5wX1W9Qqtchj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.70.0\n",
        "! pip install -q gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNT7KeDRtuWH",
        "outputId": "ec42dc5e-dad3-452b-912e-9e534f5073c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==1.70.0 in /usr/local/lib/python3.11/dist-packages (1.70.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.70.0) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.70.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.70.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.70.0) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.70.0) (2.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai==1.70.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.70.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai==1.70.0) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai==1.70.0) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.70.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.70.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.70.0) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.70.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.70.0) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.70.0) (0.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "id": "ic5uSt0quFYl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n",
        "# Kernel 1: Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Kernel 2: Define system prompt\n",
        "base_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a helpful assistant. Answer the user's questions clearly and respectfully.\"\n",
        "}\n",
        "\n",
        "# Kernel 3: Convert Gradio history to OpenAI-style messages\n",
        "def history_to_messages(history):\n",
        "    messages = [base_message]\n",
        "    for user_msg, assistant_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "    return messages\n",
        "\n",
        "# Kernel 4: Chatbot logic\n",
        "def chat_with_gpt(message, history):\n",
        "    messages = history_to_messages(history)\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    try:\n",
        "        response = client.responses.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            input=messages\n",
        "        )\n",
        "        reply = response.output_text\n",
        "    except Exception as e:\n",
        "        reply = f\"Error: {str(e)}\"\n",
        "\n",
        "    return reply\n",
        "\n",
        "# Kernel 5: Launch Gradio interface\n",
        "gr.ChatInterface(chat_with_gpt).launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "Ra5NhTjBtpGQ",
        "outputId": "28c136a8-1afc-4094-a5ac-4873d0d5d7dd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f5735dfd671b3fb0ae.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f5735dfd671b3fb0ae.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üõ°Ô∏è Add Guardrails-AI to the Model\n",
        "\n",
        "Now we will add **Guardrails-AI** to our chatbot to make it safer.\n",
        "\n",
        "With Guardrails, the model will:\n",
        "- Block bad or harmful language\n",
        "- Avoid unsafe or offensive replies\n",
        "- Show a safe message if something is not allowed\n",
        "\n",
        "This will help us see how the chatbot behaves **with safety checks** compared to the version **without Guardrails**.\n",
        "\n",
        "Let‚Äôs move to the next steps.\n"
      ],
      "metadata": {
        "id": "sHEuXgwHu8zT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJE3gRZPnmWO",
        "outputId": "1910f0a7-3094-431e-9e22-a1aab42ccd8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: guardrails-ai in /usr/local/lib/python3.11/dist-packages (0.6.5)\n",
            "Requirement already satisfied: diff-match-patch<20230431,>=20230430 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (20230430)\n",
            "Requirement already satisfied: faker<26.0.0,>=25.2.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (25.9.2)\n",
            "Requirement already satisfied: griffe<0.37.0,>=0.36.9 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (0.36.9)\n",
            "Requirement already satisfied: guardrails-api-client<0.5.0,>=0.4.0a1 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (0.4.0a1)\n",
            "Requirement already satisfied: guardrails-hub-types<0.0.5,>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (0.0.4)\n",
            "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (1.1.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (4.23.0)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (0.3.49)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.37.14 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (1.65.3)\n",
            "Requirement already satisfied: lxml<5.0.0,>=4.9.3 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (4.9.4)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.30.1 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (1.31.1)\n",
            "Requirement already satisfied: pip>=22 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (24.1.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (2.11.1)\n",
            "Requirement already satisfied: pydash<8.0.0,>=7.0.6 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (7.0.7)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (2.10.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (13.9.4)\n",
            "Requirement already satisfied: rstr<4.0.0,>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (3.2.2)\n",
            "Requirement already satisfied: semver<4.0.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (3.0.4)\n",
            "Requirement already satisfied: tenacity>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (0.9.0)\n",
            "Requirement already satisfied: typer<0.16,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-ai) (4.13.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe<0.37.0,>=0.36.9->guardrails-ai) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from guardrails-api-client<0.5.0,>=0.4.0a1->guardrails-ai) (75.2.0)\n",
            "Requirement already satisfied: urllib3<2.1.0,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from guardrails-api-client<0.5.0,>=0.4.0a1->guardrails-ai) (2.0.7)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (0.24.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (1.5.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (3.10)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (0.1.1)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (24.11.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (0.3.22)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (24.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (8.1.8)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (3.1.6)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (1.1.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (0.21.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (4.67.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.2.18)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-api~=1.15 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.31.1)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.31.1->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (5.29.4)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->guardrails-ai) (0.52b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (2.18.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.1->guardrails-ai) (2024.11.6)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16,>=0.9.0->guardrails-ai) (1.5.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.17.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.0.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->guardrails-ai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->guardrails-ai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->guardrails-ai) (0.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai) (0.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.18.3)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.30.1)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]<5.0.0,>=4.22.0->guardrails-ai) (2.9.0.20241206)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (2025.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install guardrails-ai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!guardrails configure\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1FSmO3cpnqL",
        "outputId": "5618746d-3fcb-4e18-bbc9-af26ccccdddf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enable anonymous metrics reporting? [Y/n]: y\n",
            "Do you wish to use remote inferencing? [Y/n]: y\n",
            "\n",
            "\u001b[1mEnter API Key below\u001b[0m\u001b[1m \u001b[0m\u001b[2;3mleave empty if you want to keep existing token\u001b[0m\u001b[3m \u001b[0m\n",
            "üëâ You can find your API Key at \u001b[4;94mhttps://hub.guardrailsai.com/keys\u001b[0m\n",
            "\n",
            "API Key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExODE4OTYxNjg1NjMyNDE0OTk1MCIsImFwaUtleUlkIjoiNjNkZmE2ZmMtOGRkOC00NjdmLTlkZjktMWM5YzhmOGI1OWM3Iiwic2NvcGUiOiJyZWFkOnBhY2thZ2VzIiwicGVybWlzc2lvbnMiOltdLCJpYXQiOjE3NDM2Nzg5NzUsImV4cCI6MTc1MTQ1NDk3NX0.XuH-4myNcJ0LmHUVzE65tqR1G5G3SflWG1Wqhf_o3zk\n",
            "SUCCESS:guardrails-cli:\n",
            "            Login successful.\n",
            "\n",
            "            Get started by installing our RegexMatch validator:\n",
            "            https://hub.guardrailsai.com/validator/guardrails_ai/regex_match\n",
            "\n",
            "            You can install it by running:\n",
            "            guardrails hub install hub://guardrails/regex_match\n",
            "\n",
            "            Find more validators at https://hub.guardrailsai.com\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! guardrails hub install hub://guardrails/profanity_free --quiet\n",
        "! guardrails hub install hub://guardrails/toxic_language --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqtjGyi7p0dB",
        "outputId": "62396e86-7c41-4140-8a25-6d6330d05776"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mprofanity_free...\u001b[0m\n",
            "‚úÖSuccessfully installed guardrails/profanity_free version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m0\u001b[0m!\n",
            "\n",
            "\n",
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mtoxic_language...\u001b[0m\n",
            "2025-04-04 12:36:12.860897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743770173.235317   11088 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743770173.326604   11088 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-04 12:36:14.022854: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "‚úÖSuccessfully installed guardrails/toxic_language version \u001b[1;36m0.0\u001b[0m.\u001b[1;36m2\u001b[0m!\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Step 1: Initialize Guard\n",
        "\n",
        "The `Guard` object is the core of the Guardrails-AI system. It is responsible for executing LLM calls and ensuring that the responses meet the **validation requirements** defined for the model.\n",
        "\n",
        "By initializing the guard, you set up the framework to run safety checks on both **user input** and **LLM output** before returning results to the end user.\n"
      ],
      "metadata": {
        "id": "OVLnEs-wqFz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel 1: Import required libraries\n",
        "from guardrails import Guard\n",
        "from guardrails.errors import ValidationError\n",
        "from guardrails.hub import ProfanityFree, ToxicLanguage\n",
        "import gradio as gr\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n"
      ],
      "metadata": {
        "id": "tWXRBTwPqZ9S"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from guardrails.hub import ProfanityFree, ToxicLanguage\n",
        "\n",
        "guard = Guard()\n",
        "guard.name = 'ChatBotGuard'\n",
        "guard.use_many(ProfanityFree(), ToxicLanguage())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-v7IxmHqdJf",
        "outputId": "b27296d4-2d64-4a7a-85ce-f01aa147017b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Guard(id='CEQ5OB', name='ChatBotGuard', description=None, validators=[ValidatorReference(id='guardrails/profanity_free', on='$', on_fail='exception', args=None, kwargs={}), ValidatorReference(id='guardrails/toxic_language', on='$', on_fail='exception', args=None, kwargs={'threshold': 0.5, 'validation_method': 'sentence'})], output_schema=ModelSchema(definitions=None, dependencies=None, anchor=None, ref=None, dynamic_ref=None, dynamic_anchor=None, vocabulary=None, comment=None, defs=None, prefix_items=None, items=None, contains=None, additional_properties=None, properties=None, pattern_properties=None, dependent_schemas=None, property_names=None, var_if=None, then=None, var_else=None, all_of=None, any_of=None, one_of=None, var_not=None, unevaluated_items=None, unevaluated_properties=None, multiple_of=None, maximum=None, exclusive_maximum=None, minimum=None, exclusive_minimum=None, max_length=None, min_length=None, pattern=None, max_items=None, min_items=None, unique_items=None, max_contains=None, min_contains=None, max_properties=None, min_properties=None, required=None, dependent_required=None, const=None, enum=None, type=ValidationType(anyof_schema_1_validator=None, anyof_schema_2_validator=None, actual_instance=<SimpleTypes.STRING: 'string'>, any_of_schemas={'SimpleTypes', 'List[SimpleTypes]'}), title=None, description=None, default=None, deprecated=None, read_only=None, write_only=None, examples=None, format=None, content_media_type=None, content_encoding=None, content_schema=None), history=[])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Step 2: Initialize Base Message to LLM\n",
        "\n",
        "We create a **system message** to tell the LLM how it should behave. This helps guide the chatbot's responses and sets the tone for the conversation.\n"
      ],
      "metadata": {
        "id": "35uG9Ytbqxis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel 3: Define the system/base prompt for the assistant\n",
        "base_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a helpful assistant. Answer user queries clearly and respectfully.\"\n",
        "}"
      ],
      "metadata": {
        "id": "6GKB6QpUq4n6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel 4: Convert Gradio history to OpenAI-compatible chat format\n",
        "def history_to_messages(history):\n",
        "    messages = [base_message]\n",
        "    for user_msg, assistant_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "    return messages\n"
      ],
      "metadata": {
        "id": "q5IPtt-BrTp7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel 5: Handle user input and generate safe response\n",
        "def chat_with_guardrails(message, history):\n",
        "    messages = history_to_messages(history)\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    try:\n",
        "        response = guard(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages,\n",
        "            temperature=0.7\n",
        "        )\n",
        "    except Exception as e:\n",
        "        if isinstance(e, ValidationError):\n",
        "            return \"I'm sorry, I can't answer that question.\"\n",
        "        return \"There was an error while processing your request.\"\n",
        "\n",
        "    return response.validated_output\n"
      ],
      "metadata": {
        "id": "nCJhpfJ_rY2e"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel 6: Launch the Gradio chatbot UI\n",
        "gr.ChatInterface(chat_with_guardrails).launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "yo4Htwherdi3",
        "outputId": "22642d01-6d25-4965-a29d-0daf481061a5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://48d791e6e01b141927.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://48d791e6e01b141927.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Gradio Chatbot using **Gemini + Guardrails-AI**\n"
      ],
      "metadata": {
        "id": "m3Y0ye28wmo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel 1: Import required libraries\n",
        "from guardrails import Guard\n",
        "from guardrails.errors import ValidationError\n",
        "from guardrails.hub import ProfanityFree, ToxicLanguage\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# Kernel 2: Setup Guardrails with safety checks\n",
        "guard = Guard()\n",
        "guard.name = 'ChatBotGuard'\n",
        "guard.use_many(\n",
        "    ProfanityFree(),\n",
        "    ToxicLanguage()\n",
        ")\n",
        "\n",
        "# Kernel 3: Define system prompt\n",
        "base_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a helpful assistant. Answer the user's questions clearly and respectfully.\"\n",
        "}\n",
        "\n",
        "# Kernel 4: Convert Gradio chat history to message format\n",
        "def history_to_messages(history):\n",
        "    messages = [base_message]\n",
        "    for user_msg, assistant_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "    return messages\n",
        "\n",
        "# Kernel 5: Chat logic using Guardrails + Gemini\n",
        "def chat_with_guardrails_gemini(message, history):\n",
        "    messages = history_to_messages(history)\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    try:\n",
        "        response = guard(\n",
        "            messages=messages,\n",
        "            model=\"gemini/gemini-2.5-pro-exp-03-25\",\n",
        "            temperature=0.7\n",
        "        )\n",
        "        reply = response.validated_output\n",
        "    except Exception as e:\n",
        "        if isinstance(e, ValidationError):\n",
        "            reply = \"Sorry, that question can't be answered due to safety filters.\"\n",
        "        else:\n",
        "            reply = f\"Error: {str(e)}\"\n",
        "    return reply\n",
        "\n",
        "# Kernel 6: Launch Gradio chatbot\n",
        "gr.ChatInterface(chat_with_guardrails_gemini).launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "Eq_bNPAowmR1",
        "outputId": "199b4b6d-2397-4d5b-b6e0-f66052136eb7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://13344ad86352167113.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://13344ad86352167113.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}